\documentclass[11pt]{article}

% =========================================================================
% document style changes
% =========================================================================

\usepackage{amsmath}                    % AMS math packages
\usepackage{amssymb}                    %
\usepackage[]{graphpap}
\usepackage[T1]{fontenc}                % for \mathrm{}
\usepackage{courier}                    % for \texttt{}
\usepackage{bbm}                        % for \mathbbm{1} (indicator function)
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[font={small}]{caption}
\usepackage{subcaption}

%\setlength{\parskip}{\baselineskip}     % skip line following paragraphs
%\pagestyle{empty}                       % No page numbers
\setlength{\topmargin}{-.5in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{.125in}
\setlength{\textwidth}{6.25in}

\newcommand{\spc}{\vspace{0.25in}}      % Shortcut commands
\newcommand{\ds}{\displaystyle}         %\newcommand{\ds}[1]{\displaystyle{#1}}
\newcommand{\ra}{\rightarrow}
\newcommand{\Cov}{\mathrm{Cov}}
\DeclareMathOperator*{\argmax}{arg\!\max}
\DeclareMathOperator*{\argmin}{arg\!\min}

\begin{document}                        % This is where the document begins

\title{A Technique for Inter-Catalogue Recommendations}
\author{Bill Chickering and Jamie Irvine\\
CS 399 with Anand Rajaraman\\
Stanford University}
\renewcommand{\today}{June 12, 2014}
\maketitle

\section*{Abstract}
\emph{Abstracts are abstract. They confront you. There was a reviewer a
while back who wrote that my papers didn't have any beginning or any end. He
didn't mean it as a compliment, but it was.} --Jackson Pollock

\section*{Introduction}
Recommender systems have become a critical component for online retailers in
nearly every space of retail. Whether the products are movies or clothes or
research articles, large retailers tend to have a way of recommending products
based on others previously viewed or purchased products.

These recomendations usually come from a combination of content-based
information and collaborative filtering techniques. The content-based approach
uses information about the products, such as category or description, to connect
related products. Alone, however, this approach is usually unable to capture the
subtle ways products may actually be related. Collaborative filtering, on the
other hand, learns relations from aggregated user behavior.  Such
recommendations often have explicit titles like ``People who liked this product
also liked these products''. Collaborative filtering has proven to be effective
at recommending products, given that there is a large amount of user data.

Since each retailer only has user data within their site, these user-based
recommender systems are good at learning connections between products in one
catalogue, but they do little to learn how these products may relate to other
products outside of the retailer's inventory. This type of information is
valuable in a number of situations. Retailers such as department stores want to
know which new products make the most sense to add to their catalogue.  This is
essentially a question of recommending the best outside products based on
current in-house ones. In a different scenario, two retailers may merge and want
to construct a unified recommender system between the two catalgoues.

We model this as a problem of connecting recommender systems between disjoint
catalogues. The goal is to construct good recommendations from products in one
catalogue to products in the other. These catalogues may be from competing
retailers who don't have access to each other's user data. Thus, to keep the
problem general, we do not assume that we have any private internal data.
Instead, we constrain ourselves to use only the product information and internal
recommendations that are posted publicly to all users.

This is a difficult problem. What makes a good recommendation is hard to define
objectively. One can say that if users co-purchase or co-view products together,
then these products should be recommended for each other. But without any user
data, it is hard to simulate this kind of user behavior. The connections between
co-viewed products are often less tangible than simple metrics of similarity. 

In this paper, we outline a technique that leverages the intra-catalogue
recommendations of each catalogue to construct inter-catalgoue recommendations
between them. We focus on the clothing retailers, since they tend to all have
public recommendations for each product. Specifically, the data used for this
project was all collected from the public product webpages for Macy's.

\section*{Data}
For this work, we utilize most of the product catalog of Macy's as presented at
www.macys.com. This catalog consists of numerous categories of mostly apparel,
each of which includes hundreds to thousands of items. We confine this study to
the 49 categories within the two main parent categories ``Men'' and ``Women''
that contain at least 200 items that are associated to multiple brands. In
total, this dataset consists of 66,071 items in 49 categories with some items
listed in multiple categories. For each item we record its description, all
categories within which it is listed, and its associated recommendations. By
{\em recommendations}, we are referring to the items---there are typically four
in the case of Macy's---that are displayed on a product's details page under the
heading ``Customers Also Shopped''. The presence of these recommendations
implicitly forms a directed graph over the product catalogue, in which the nodes
are the {\em products} or {\em items} and the edges are the {\em
recommendations}. These graphs play a central role in this study.

\section*{Experiments}
Our goal is to develop methods for connecting initially disconnected
recommendation graphs. That is, given two online retailers, each with a website
that displays items in their respective catalogue, where each item is
accompanied by several {\em recommended} items, we are presented with two
disconnected recommendation graphs. We would like to associate ``good''
recommendations for items in one catalogue with items in the other catalogue. In
this way, we are effectively introducing edges that connect the two
recommendation graphs. Formulating this inter-retailer recommendations problem
in terms of graphs offers insight in how to evaluate our recommendation choices
as well as how to choose ``good'' recommendations. Leveraging the graph
structure for evaluation is discussed in this section while exploiting the graph
for the purpose of making better recommendations is addressed in a subsequent
section.

To evaluate our inter-retailer recommendation methods, we simulate the problem
by randomly partitioning Macy's products into two disjoint sets. Given the
original recommendation graph, each partition therefore corresponds to a graph
cut. Our premise is that an ideal recommendation method could guess the edges in
this cut with high precision and recall.

Precision, recall, and $F_1$ score are well-known metrics for evaluating
prediction algorithms.  Together, these metrics capture how effective one is at
guessing the items within a target set. Consider a typical Macy's category, for
example, Dresses, which contains about $3,500$ items. Randomly partitioning
results in two sets, each with about $1,750$ items. For each interset
recommendation, we must therefore choose two nodes, one from each of the two
sets. That is, we have $1,750 \times 1,750 > 3 \times 10^6$ choices. Choosing
the correct edges is a formidable challenge to say the least. It is therefore
worth asking: Are some prediction errors better or worse than others? The graph
nature of our problem reveals that the answer is yes. It is better to guess an
edge that connects two items that are separated by two edges in the original
graph than to connect two items that are separated by five edges. We therefore
introduce the notion of {\em 2-precision}, {\em 2-recall}, and {\em 2-}$F_1$ are
defined as

\begin{align}
\text{\em 2-precision} &= \left|\left\{(u,v) \in P | d_L(u,v) \leq 2
\right\}\right|
\\\text{\em 2-recall} &= \left|\left\{(u,v) \in L | d_P(u,v) \leq 2
\right\}\right|
\\\text{\em 2-}F_1 &= 2\cdot \frac{\text{\em 2-precision} \cdot \text{\em
2-recall}}{\text{\em 2-precision} + \text{\em 2-recall}}
\end{align}
where $P$ is the set of predicted edges, $L$ is the set of lost edges (i.e.
those in the cut resulting from the partition), and $d_L(u,v)$ is the distance
between nodes $u$ and $v$ in the original unpartitioned graph, and $d_P(u,v)$ is
the distance between nodes $u$ and $v$ in the new graph formed using the
predicted edges in $P$.


In these experiments, we perform the partitioning on a per-category basis. 










Next, for each category (e.g. Women Activewear, Men Dress Shirts, etc.), we
partition the products into two disjoint sets to simulate the catalogues of two
distinct retailers. Importantly, we ensure that the items of each brand are
entirely within a single partition. In this way, we preclude the easiest method
of associating two items: recongnizing a common brand. This is done to increase
the difficulty and realism of our experiment.

To make this partition realistic, we
ensure that no brand has products in both catalogue. We remove all
recommendations between products from one partition to the other, thus creating
two entirely disjoint and unconnected catalogues.

Now, we can use this partition to test a method for constructing recommendation.
For each product in each of the two catalogues, we use our recommender to choose
which products to recommend in the other catalogue. To make this problem
slightly simpler, we restrict recommendations to be within the same category,
such as Women's Dresses or Men's Shoes. In this way, connecting up entire
catalogues becomes a series of smaller tasks of connecting the products in each
category.

For this experiment, we have access to all of the true inter-catalogue
recommendations from the unpartitioned Macy's catalogue. Although we don't use
this information for constructing recommendations between the partitioned
graphs, we can use the original recommendations as a gold-standard to test the
construct ones. Using these gold-standard recommendations, we can evaluate the
precision and recall of the predicted inter-catalogue recommendations.

For the Macy's catalogue, each product has four recommendations. This means that
on average, there are only two gold-standard inter-catalogue recommendations
from each product. Since there potentially are other good
recommendations besides these two, we create another metric that expands our
notion of a correct recommendation. Here we extend the definition of a good
connection to include recommendations of recommendations in the original
catalgoue. For example, if Product B is recommended for Product A and 
Product C is recommended for Product B, we say that Product C is still a good 
recommendation for Product A. This leads us to the notions of 2-precision and 
2-recall.

For a rigorous definition of 2-precision and 2-recall, we first propose the idea
of a recommender graph. A recommender graph has products as nodes and
recommendations as edges. If Product B is recommended for Product A, there is an
edge between node A and node B in the graph. For simplicity, we define our
recommender graphs to be undirected, but we note that they could be interpreted
as directed graphs as well. With this terminology, our problem becomes one of
connecting two disjoint recommender graphs by adding edges between them. 

2-precision is defined as the percentage of predicted edges that are within two
steps in the original, unpartitioned recommender graph. In the example above,
Product A and Product C are within two steps in the original graph, thus an edge
between A and C is considered correct and increases precision. 2-recall is
defined analogously. Here, we consider the new recommender graph, consisting of
the two partitioned recommender graphs connected by the predicted
recommendations. 2-recall is then the percentage of gold-standard edges that 
are within two steps in the predicted recommender graph.

\section*{Constructing Recommendations}
Recommending products between different catalogues is a daunting task.
Traditionally, constructing recommendations is done by leveraging user behavior
using a collaborative filtering technique. At the most basic level, If a large
number of users view or purchase both Product A and Product B, the two are
connected with a recommendation. Unfortunately, for this problem there are no
user data between the two unconnected catalogues.

\subsection*{Content Based Similarity}
A simple approach for this problem is to recommend products that have similar
content. The products have both a short and long description which we can use to
construct a textual content-based similarity function. To do this, we represent each
product as a vector of tf-idf values for each word in its descriptions. By using
tf-idf, we reduce the relevance of frequent words and boost that of rare ones,
thus emphasizing the more words that are more descriptive of a product versus
all the other ones in the catalogue. The vector representations are the length 
of the dictionary, with zeros for all words not in the descriptions. To determine
the content-based similarity between two products, we simply take the dot 
product of the two tf-idf vectors. 

Mathematical definition of content-based similairty...

This similarity function yields the following recommendation algorithm: For each
product, scan through the products in the opposite catalogue and calculate the
content-based similarity score with each. Predict recommendations between this
product and each of the top k most similar products in the other catalogue.

While this is a quadric-time operation, it can be sped up using matrix
multiplication. Furthermore, the division into different categories reduces the
impact of the quadratic effect. For instance, Macy's entire catalogue consists
of over ONE MILLION products, making a quadratic time algorithm infeasible.
However, the average category has 2000 products, which is significantly more
manageable.

\subsection*{Leveraging Recommendations}
The simple content-based recommender is limited for a few reasons. It attempts
to capture similarity between products by looking exclusively at the words used
to describe them. One limitation here is that different brands often use
different vocabularies to describe the same thing, such as ``off-white'' versus
``ivory.'' Another issue is that the best recommendation for a product is often
more subtle than simply the most similar other product. For instance, a striped
black-and-white button-up shirt may be an excellent recommendation for a blue
checkered button-up, even though the two have ostensibly different patterns and
colors. 

We can alleviate some of these issues by leveraging the recommendations we
already have between products in the same catalogue. One approach is to consider
the neighborhood of each product in their recommender graphs, that is a product
along with each of the products it is recommended with in its own catalogue.
When comparing two products in different catalogues, we use the same tf-idf
similarity function from before to compare each pair of products between the two
neighborhoods. Mathematically, we construct the similarity function
NeighborhoodSimilarity defined as:

NeighborhoodSimilarity definition...

Assuming the recommendations are good within each of the catalogues, this
approach can help both of the issues brought up with the simple content-based
recommender. Since a neighborhood has many products, each describing themselves
with potentially different vocabularies, there is a higher likelihood that the
neighborhood contains multiple synonyms and thus decreases the potential of
missing due to different vocabularies. Furthermore, the recommender graphs
capture some of the user behavior not expressed in product descriptions.
Re-examining the button-up shirts example, the neighborhood of a striped shirt may
include checkered shirts and vise-versa. In this case, the two neighborhoods
would share the words ``striped'' and ``checkered'' and thus the two previously
dissimilar shirts would now be considered similar.

\subsection*{Latent Concepts}
Although the NeighborhoodSimilarity method alleviates some of the issues of
content-similarity, it is still fundamentally based on the descriptions of
products. Ideally, we would like to recommend products that are similar in more
fundamental ways than their descriptions. If a dress is elegant and modern, we
want to recommend another dress that is elegant and modern, even if those words
are not explicitly in the descriptions of either. These types of latent concepts
can be intangible and difficult to define. However, if we assume that the given
recommendations are truly based on these underlying concepts, we can attempt to 
learn the latent concepts by analyzing the graphs.

One approach to learning latent concepts is Latent Semantic Indexing or LSI. In
traditional LSI, used for Natural Language Processing, concepts are learned over 
a dataset consists of documents, each containing terms. Singular Value 
Decomposition (SVD) is applied to the term-document matrix and the resulting 
three matrices are truncated to the number of latent concepts desired. The 
resulting left matrix is a term-concept matrix, where each row is a term 
represented in concept space.

We apply the same idea to the product catalogues in order to learn the latent
concepts of each product. Analogous to documents containing terms, we look at
user sessions which contain the products viewed by that user. Although we do not
have actual user data, we can simulate user sessions by traversing the
recommender graphs. 

To simulate user sessions, we use a random walk technique on the graph. We
assume that the edges of a recommender graph indeed connect products that a user
would co-view. Thus traversing the edges of a graph is a reasonable simulation
of a user session. For details on these random walks, see the section titled
Random Walks in the Implementation section below.

With the simulated user sessions, we can implement LSI on each of the catalogues
and learn the latent concepts for each product. We want to find the products from
one catalogue that are most similar in latent concept space to products in the
other catalogue. However, since the user sessions (random walks) include
products either from one catalogue or the other, we learn different latent 
spaces for each catalogue. The problem is therefore reduced to finding a mapping
from one concept space to the other. If this can be accomplished, we
can map the concepts of each product into concepts of the other catalogue. This
creates a concept representation in the other catalouge's concept space, meaning
that we can then find nearest neighbors in that catalogue's concept space. 

To map concepts from one catalogue to another, we must revisit
content-similarity. In this case, we represent each concept as a vector of
words. To calculate the value for a word w for concept c, we sum the tf-idf
value of w across all products, weighted by how much that product is represented
by concept c. Thus if a concept could be described as formal, the words
``sophisticated'' or ``official'' may have high value since products with those
descriptive words would also have a high amount of the formal concept in their
concept representations. Using these vector representation for concepts, we can
simply calculate the dot product between two concepts to find how similar they
are, in a relative sense. This can be used to create a mapping from one set of
concepts to another.

In all, the LSI approach consists of four phases. First we simulate user
sessions by randomly walking the recommender graphs. Second we learn concept
spaces for products in each catalogue via SVD. Third we find a mapping between
concept spaces. And lastly we find the nearest neighbors of each product in the
other catalogue's concept space. These nearest neighbors are the predicted
recommendations between products in different catalogues.

\section*{Implementation}
Here are some of the details for implementing the LSI method for constructing
inter-catalogue recommendations.

\subsection*{Random Walks}
Topic-specific random walk starting from each product
Parameters needed and the ones chosen

\subsection*{Singular Value Decomposition}
Subtract mean before hand (why this is good here but not usually)
Number of concepts to use

\subsection*{Concept Mapping}
Issue of normalization - normalize each column (L1 and L2), orthogonalize, map
to sphere
Bigrams

\subsection*{Popularity}
Important - definition, popularity breakdown of graphs
Model of popularity and similarity
Removing popularity from random walks
Adding popularity to nearest neighbors / modifying number of predictions
Baseline popularity methods
Number of products mapped to in different methods

\section*{Performance}
\subsection*{Baselines}
Random, tfidf, one model (explain), most popular

\subsection*{Results}
\begin{center}
\begin{tabular}{ | l | c | c | c |}
\hline
Normalization & 2-Precision & 2-Recall & 2-F1 Score \\ \hline\hline
L1 normalize each column of matrix &&&\\ \hline
L2 normalize each column of matrix &&&\\ \hline
Orthogonalize matrix &&&\\ \hline
Normalize products in concept space &&&\\ \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ | l | c | c | c |}
\hline
Method & 2-Precision & 2-Recall & 2-F1 Score \\ \hline\hline
Random &&&\\ \hline
Simple Content-based &&&\\ \hline
Neighborhood Content-based &&&\\ \hline
One Model &&&\\ \hline
Popularity Baselines &&&\\ \hline
\hspace*{0.5cm} Map to most popular &&&\\ \hline
\hspace*{0.5cm} Random concept mapping &&&\\ \hline
LSI &&&\\ \hline
\hspace*{0.5cm} + Normalized concept mapping &&&\\ \hline
\hspace*{0.5cm} + Zero-mean SVD &&&\\ \hline
\hspace*{0.5cm} + Bigrams from descriptions &&&\\ \hline
\hspace*{0.5cm} + Discounted popularity from random walks &&&\\ \hline
\hspace*{0.5cm} + Popularity for finding nearest neighbors &&&\\ \hline
\hspace*{0.5cm} + Number of recommendations based on popularity &&&\\ \hline

\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ | l | c | c | c | c | c | c |}
\hline
Category & 1-Precision & 1-Recall & 1-F1 Score & 2-Precision & 2-Recall & 2-F1 Score \\ \hline\hline
Men's Shirts &&&&&&\\ \hline
Women's Dresses &&&&&&\\ \hline
\end{tabular}
\end{center}

Analysis/descriptions for each of these tables

\section*{Conclusion}
We tried a few different methods for creating recommendations between
retailers. We find that this one is the best.
The last method uses an unconventional approach that adds some value in the
process. These results could be used in real life for these problems, since all
of the data exists.

\section*{Future Work}
Incorporate more data such as customer reviews. Use computer vision techniques.
Focus more on popular hubs model.

\begin{thebibliography}{9}

\end{thebibliography}

\end{document}
