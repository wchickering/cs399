\documentclass[11pt]{article}

% =========================================================================
% document style changes
% =========================================================================

\usepackage{amsmath}                    % AMS math packages
\usepackage{amssymb}                    %
\usepackage[]{graphpap}
\usepackage[T1]{fontenc}                % for \mathrm{}
\usepackage{courier}                    % for \texttt{}
\usepackage{bbm}                        % for \mathbbm{1} (indicator function)
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[font={small}]{caption}
\usepackage{subcaption}

%\setlength{\parskip}{\baselineskip}     % skip line following paragraphs
%\pagestyle{empty}                       % No page numbers
\setlength{\topmargin}{-.5in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{.125in}
\setlength{\textwidth}{6.25in}

\newcommand{\spc}{\vspace{0.25in}}      % Shortcut commands
\newcommand{\ds}{\displaystyle}         %\newcommand{\ds}[1]{\displaystyle{#1}}
\newcommand{\ra}{\rightarrow}
\newcommand{\Cov}{\mathrm{Cov}}
\DeclareMathOperator*{\argmax}{arg\!\max}
\DeclareMathOperator*{\argmin}{arg\!\min}

\begin{document}                        % This is where the document begins

\title{A Technique for Inter-Catalogue Recommendations}
\author{Bill Chickering and Jamie Irvine\\
CS 399 with Anand Rajaraman\\
Stanford University}
\renewcommand{\today}{June 11, 2014}
\maketitle

\section*{Abstract}
\emph{Abstracts are abstract. They confront you. There was a reviewer a
while back who wrote that my papers didn't have any beginning or any end. He
didn't mean it as a compliment, but it was.} --Jackson Pollock

\section*{Introduction}
A key challenge in retail is choosing which items to incorporate into one's
collection of offered goods and services.  Several factors must be considered
including the number of product lines, the variety of products in each line, as
well as the consistency and relationships between products. In this study, we
focus on the latter and explore techniques that leverage publicly available
product recommendation information for the purpose of improving product
assortment decisions.

It has become standard practice for online retailers to recommend one or more
products to prospective customers who have viewed or purchased an item. These
recommendations are generally derived from either content-based approaches or
via collaborative filtering, utilizing data-mining techniques \cite{Ricci2011}.
Importantly, these recommendations provide a source of publicly available
business intelligence by relating the items in their respective catalogue.
Specifically, these online recommendations logically form a directed graph in
which the nodes are products and an edge pointing from item $A$ to item $B$
indicates that customers who view or purchase item $A$ are recommended item $B$.
Given the recommendation graphs of two or more distinct retailers, our goal is
to determine new, meaningful edges that connect these graphs in a way that would
allow one retailer to relate their products to those of another.

Content-based approaches and collaborative filtering have distinct challenges,
and therefore, offer unique advantages as solutions to the general recommender
problem.  An effective content-based solution requires a detailed analysis of
item and/or user profile information. A well-known, successful example is the
Music Genome Project, which powers the music service Pandora \cite{mgp}. Such an
approach can be difficult, however, since necessary user and/or item information
might be unavailable or the required domain expertise might be too costly. The
alternative method is that of collaborative filtering (CF), which leverages the
correlations within user purchase or rating data. The idea is that two items
that tend to be purchased or highly rated by the same users can be considered
related such that if a new user likes one they will probably like the other. A
key advantage of this approach is that it effectively outsources the
recommendation problem to the retailer's customers. Online retailers such as
Amazon.com and Netflix are known to employ recommender systems based on CF
\cite{Koren2009}. At the same time, CF notoriously suffers from what is known as
the {\em cold start}---without user-item data of sufficient quantity and
diversity these systems fail to yield accurate results.

The problem we examine here is, in many ways, more difficult than that of the
standard recommendation problem. Essentially, we would like to recommend an item
from retailer $X$ based solely on a known preference for an item from retailer
$Y$. For the product assortment problem, user purchase and rating information is
presumably known for one of the two retailers. In this study, however, we
simplify the scenario by making it symmetric. No user purchase or rating
information will be leveraged for either retailer. Instead, the algorithms
investigated here are limited to publicly available information, which includes
product descriptions and online product recommendations. Given the lack of
inter-retailer user data, a traditional CF technique is ostensibly not
applicable. Instead, we propose a new technique that leverages the recomender
graphs as well as the content from each product to create inter-retail
recommendations.

In this report. . . . (BREIFLY OUTLINE THE PAPER HERE.)



\section*{Data}
For this work, we utilize most of the product catalogue of Macy's as presented
at www.macys.com. This catalogue consists of numerous categories of mostly
apparel, each of which includes hundreds to thousands of items. We confine this
study to the 49 categories within the two main parent categories ``Men'' and
``Women'' that contain at least 200 items that are associated to multiple
brands. In total, this dataset consists of 66,071 items in 49 categories with
some items listed in multiple categories. For each item we record its
description, all categories within which it is listed, and its associated
recommendations. By {\em recommendations}, we are referring to the items---there
are typically four in the case of Macy's---that are displayed on a product's
details page under the heading ``Customers Also Shopped''. The presence of these
recommendations implicitly forms a directed graph over the product catalogue, in
which the nodes are the {\em products} or {\em items} and the edges are the {\em
recommendations}. We call these types of graphs {\em recommendation graphs} and
they play a central role in this study.

To simplify the present study, we transform these directed recommendation graphs
into undirected graphs. This is consistently done using the following policy:
For each pair of items connected by a single directed edge, replace this edge
with a single undirected edge. For each pair of items connected by two
oppositely directed edges, replace both edges with a single undirected edge.
Working with undirected graphs simplifies both the prediction algorithms as well
as the evaluation of their performance. At the same time, by ignoring the
directionality of the original edges, we are discarding important information.
It would therefore be worthwhile to consider the more difficult problem of
connecting directed recommendation graphs in a future study.

\section*{Experiment}
Our goal is to develop methods for connecting initially disconnected
recommendation graphs. Given two online retailers, each with a website
displaying their items along with several other {\em recommended} items from
their catalogue, we are presented with two disconnected recommendation graphs.
We would like to associate ``good'' recommendations for items in one catalogue
with items in the other catalogue. In this way, we are effectively introducing
edges that connect the two recommendation graphs. Formulating this
inter-retailer recommendations problem in terms of graphs offers insight in how
to evaluate our recommendation choices as well as how to choose ``good''
recommendations.  Leveraging the graph structure for evaluation is discussed in
this section while exploiting the graph for the purpose of making better
recommendations is addressed in a subsequent section.

To evaluate our inter-retailer recommendation methods, we simulate the problem
by randomly partitioning Macy's products into two disjoint sets. Given the
original recommendation graph, each partition therefore corresponds to a graph
cut. The edges in this graph cut form a {\em gold-standard} since it is assumed
that these are indeed ``good'' recommendations. Our premise is that an ideal
recommendation method could guess these gold-standard edges with high precision
and recall.

For each experiment, we choose a particular category from the Macy's catalogue
(e.g. Women Activewear, Men Dress Shirts, etc.). Since most recommendations are
between items in a common category, very few, though some, edges are lost by
confining ourselves to an individual category. Edges that are lost in this way
do not participate in the experiment and are not considered during evaluation.
Also, we randomly partition the items in the category such that all items
associated with a particular brand are entirely within a single partition. In
this way, we preclude the easiest method of associating two items: recognizing
a common brand. This is done to increase the difficulty and realism of the
experiments.

Having partitioned the graph of items, we now predict recommendation relations
between items across the partitions. Each prediction algorithm has knowledge of
all items in each partition, including their descriptions and all
recommendations (i.e.  edges) between items in a common partition. In addition,
the algorithms may exploit the fact that most items listed at www.macys.com are
accompanied by four recommended items. The algorithms do not have knowledge of
the recommendations (i.e. edges) between items across partitions. Each
prediction algorithm is then free to choose an arbitrary number of edges as long
as 1) the items connected by the predicted edge are not in the same partition
and 2) at most one edge is predicted for a particular pair of items.

\section*{Evaluation}
Precision, recall, and $F_1$ score are well-known metrics for evaluating
prediction algorithms \cite{Powers2011}. Together, these metrics capture how
effective one is at guessing items within a target set. Consider a typical
Macy's category, for example, Dresses, which contains approximately $3,500$
items. Suppose that randomly partitioning results in two sets, each with about
$1,750$ items. For each interset recommendation, we must therefore choose two
nodes, one from each of the two sets. That is, we have $1,750 \times 1,750 > 3
\times 10^6$ choices. Choosing the correct edges is a formidable challenge to
say the least. It is therefore worth asking: Are some prediction errors better
or worse than others?

The graph nature of our problem reveals that the answer to this question is
yes. For instance, it is better to guess an edge that connects two items that
are separated by two edges in the original graph than to connect two items that
are separated by five edges. We therefore introduce the notion of {\em
2-precision}, {\em 2-recall}, and {\em 2-}$F_1$ are defined as

\begin{align}
\text{\em 2-precision} &= \left|\left\{(u,v) \in P | d_L(u,v) \leq 2
\right\}\right|
\\\text{\em 2-recall} &= \left|\left\{(u,v) \in L | d_P(u,v) \leq 2
\right\}\right|
\\\text{\em 2-}F_1 &= 2\cdot \frac{\text{\em 2-precision} \cdot \text{\em
2-recall}}{\text{\em 2-precision} + \text{\em 2-recall}}
\end{align}
where $P$ is the set of predicted edges, $L$ is the set of lost edges (i.e.
those in the cut resulting from the partition), $d_L(u,v)$ is the shortest
distance between nodes $u$ and $v$ in the original unpartitioned graph, and 
$d_P(u,v)$ is the shortest distance between nodes $u$ and $v$ in the new graph 
formed using the predicted edges in $P$.

The following definitions will prove helpful. Let $G$ be the original
unpartitioned graph formed from the partitions together with the lost edges $L$
and let $G^{\prime}$ be the new graph formed from the partitions together with
the predicted edges $P$.  In the context of {\em 2-precision}, an edge in $P$
that connects items $A$ and $B$ is considered correct if 1) $A$ and $B$ share an
edge within $G$ or 2) $A$ shares an edge with a direct neighbor of $B$ within
$G$ or 3) a direct neighbor of $A$ shares an edge with $B$ within $G$.
Meanwhile, in the context of {\em 2-recall}, an edge in $L$ that connects items
$C$ and $D$ is considered recalled if 1) $C$ and $D$ share an edge within
$G^{\prime}$ or 2) $C$ shares an edge with a direct neighbor of $D$ within
$G^{\prime}$ or 3) a direct neighbor of $C$ shares an edge with $D$ within
$G^{\prime}$.

We use both traditional precision, recall, and $F_1$ score as well as {\em
2-precision}, {\em 2-recall}, and {\em 2-}$F_1$ score to evaluate our prediction
algorithms. Including the latter set of metrics provides a more complete picture
of algorithmic performance. These additional metrics also accommodate the fact
that the edges of $G$ do not necessarily correspond to the only or even the best
item-item recommendations. We therefore suggest that while traditional precision
and recall are relevant metrics, relative performance as measured by {\em
2-precision} and {\em 2-recall} better captures the effectiveness of these
prediction algorithms.

\section*{Algorithms}

\subsection*{\em ContentBasedRecommender}
The most straightforward approach to inter-retailer recommendations is a
content-based approach. Retail products typically have a description that can be
used to construct a textual content-based similarity function. One can then
consider all pairwise product combinations across retailers, but within a common
category, and choose recommendations between items with sufficiently high
similarity.

Toward this end, we borrow the concept of {\em Term Frequency--Inverse Document
Frequency} (tfidf) from the information retrieval community. In using tf-idf, we
are considering each item description to be a ``document''.  Thus we define the
tfidf of a term $t$ within an item description $d$ that's part of a combined
category catalogue $C$ (i.e. the catalogue consisting of all items in both
retailers' catalogues for a particular common category) as
\begin{align}
\mathrm{tfidf}(t,d,C) = f(t,d) \times
\mathrm{log}\frac{\left|C\right|}{\left|\left\{d \in C : t \in
d\right\}\right|},
\end{align}
where $f(t,d)$ is the number of occurances of term $t$ in item description $d$,
and $\left|\left\{d \in C : t \in d\right\}\right|$ is the number of items in
$C$ with descriptions containing at least one instance of term $t$.  We can now
represent each item as a sparse vector of tfidf values, which has a nonzero
element for each term in its description.

A key feature of tfidf that is useful here is how terms that appear in many item
description are discounted via the idf factor. Conversely, rare terms will have
larger tfidf values. We now define the $ContentSimilarity$ between items $A$ and
$B$ as
\begin{align}
ContentSimilarity(A,B) = v_A \cdot v_B
\end{align}
where $v_A$ and $v_B$ are the tfidf vector representations of items $A$ and $B$,
respectively.

Using this content-based similarity function, we construct the following
inter-catalogue recommendation algorithm, which we call the $ContentBased
Recommender$. For each product in each catalogue, calculate its
$ContentSimilarity$ with each and every product in the other catalogue. Predict
recommendations between this product and the top two most similar products in
the other catalogue. The reason for choosing the top two most similar products
originates from our knowledge that most items listed on www.macy.com are
associated with four recommended items. Thus, by choosing two edges per item per
partition, we will predict approximately the same number of edges that were lost
during the partition. This relatively straightforward algorithm serves as a
baseline against which to compare the performance of other more sophisticated
algorithms.

The $ContentBasedRecommender$ deserves a few more comments. For starters, the
use of an inner product, which is bounded only by the length of the tfidf
vectors, is not fundamental to this algorithm. A bounded similarity function
such as cosine similarity could have been used instead without qualitatively
changing the results discussed later in this report. This is because the lengths
of the product descriptions have little variation, and therefore, the vector
norms of the tfidf vectors have little variation. Finally, we mention that while
this algorithm is quadratic in the number items, it can be executed relatively
efficiently using matrix multiplication, which trades time for space (i.e.
memory). Furthermore, confining our experiments to individual categories reduces
the impact of the quadratic time complexity. In our case, Macy's entire
catalogue consists of over $60,000$ products, making a quadratic time algorithm
challenging. However, the average category has about two thousand products,
which is a significantly more manageable.

\subsection*{Leveraging Recommendations}
This $ContentBased Recommender$ is limited in a few ways. For one, it attempts
to capture similarity between products by looking exclusively at the words in
their descriptions. One issue here is that brands tend to use different 
vocabularies to describe the their products. One may refer to a color as 
``off-white'' while another calls the same color ``ivory.'' To the $ContentBased
Recommender$, these words are as different as ``black'' and ``white.'' Another 
issue is that the best recommendation for a product may not be the most similar
product. For instance, a striped black-and-white button-up shirt could be 
an excellent recommendation for a blue checkered button-up, even though the two 
have ostensibly different descriptions.

We can alleviate some of these issues by leveraging the recommendations we
already have between products within the same catalogue. One approach is to 
consider the {\em neighborhood} of each product, that is a product along 
with each of its neighbors in the recommendation graph. With this, we create a
new similarity function called $NeighborhoodSimilarity$. This function uses the 
$ContentSimilarity$ function from before to compare every pair of products 
between the two neighborhoods:

\begin{align}
NeighborhoodSimilarity(P_1, P_2) &= \sum\limits_{S\in
Neigh(P_1)}\sum\limits_{T\in Neigh(P_2)}
ContentSimilarity(S, T) 
\end{align}
where $Neigh(P)$ is the neighborhood of Product P. We use this similarity
function to construct the $Neighborhood Recommender$, which is analgous to the
$ContentBased Recommender$.

Assuming that the edges of the recommendation graphs are in fact good
recommendations, this approach can help both of the issues brought up with the
$ContentBased Recommender$. Since a neighborhood has many products, each 
describing themselves using potentially different vocabularies, there is a 
higher likelihood that the neighborhood contains multiple synonyms for the
underlying concept that describes their similarity. This decreases the
likelihood of false negatives due to different vocabularies. Furthermore,
the recommendation graphs capture some of the user behavior that cannot be
expressed in product descriptions. Re-examining the button-up shirts example, 
the neighborhood of a striped shirt may include checkered shirts and vise-versa.
In this case, the two neighborhoods would share the words ``striped'' and 
``checkered'' and thus the two previously dissimilar shirts would now correctly
be deemed similar.

\subsection*{Latent Concepts}
Although the $Neighborhood Recommender$ alleviates some of the issues of 
content-based similarity, it is still primarily based on the descriptions of
products. Ideally, we would like to recommend products that are similar in more
fundamental ways. If a dress is elegant and modern, we would like to recommend 
another dress that is elegant and modern, even if those words are not explicitly
in the descriptions of either. These types of underlying concepts can be 
intangible and difficult to define. However, if we assume that the given
recommendations are truly based on such latent concepts, we can attempt to learn 
them by analyzing the graphs.

One common approach to learning latent semantic factors is to apply {\em
Singular Value Decomposition} (SVD) to a matrix of user data, as in many 
collaborative filtering methods [CITE]. This method requires a product-user
matrix, where each column represents the products viewed in a user session.
After SVD is applied to the matrix, the resulting three matrices are truncated
to the number of latent concepts desired. The resulting left matrix becomes a
product-concept matrix, where each row is a product represented in 
{\em concept-space}

In this experiment, there is no such product-user matrix to apply SVD.
However, since the recommendation graphs are constructed from user behavior,
we can use them to simulate user sessions. To do this, we use a random walk 
technique on the graph. We assume that the edges of a recommendation graph 
indeed connect products that a user would co-view. Thus traversing the edges 
of a graph is a reasonable simulation of a user session. For details on these
random walks, see the section titled Random Walks in the Implementation section
below.

With simulated user data for each recommendation graph, we can apply SVD on the
matrices from the two catalogues to learn the latent concepts for each product.
We want to find the products from one catalogue that are most similar in latent
concept-space to products in the other catalogue. However, since each user
session/random walk includes products exclusively from one of the catalogues, we
learn different latent spaces for each catalogue. Note that this would be an
issue even if we had the actual user data used to construct the two
recommendation graphs. The problem therefore becomes finding a mapping from one
concept-space to the other. If this can be accomplished, we can represent each
product of one catalogue in terms of concepts of the other catalogue and then
find the nearest neighbors of that catalogue by distance in concept-space. 

To map concepts from one catalogue to another, we must revisit
content-similarity. In this case, we represent each concept (rather than each
product) as a vector of words. To represent a concept in terms of the
vocabulary, we calculate the value of each word by summing the
tf-idf of that word for each product $P$, weighted by the projection of $P$ on
our concept in concept-space. That is:

\begin{align}
C_w = \sum\limits_{P} \text{tfidf}(w)\times P_C
\end{align}
where $C_w$ is the value of concept $C$'s representation in dimension $w$, the sum 
is over all products $P$ in the catalgoue, tfidf($w$) is the tf-idf score for
word $w$, and $P_C$ is the projection of $P$ on concpet $C$'s axis in
concept-space.

Now for a concept such as {\em formal}, the words ``sophisticated'' or
``official'' may have high value since products with those descriptive words
would also have a large amount of the {\em formal} concept in their
concept-representations. Using these vector representation, we calculate the
inner product between two concepts to find how similar they are to eachother.
This can be used to create a mapping from one set of concepts to another as it
captures the relative breakdown of a concept from one space to the concepts in the
other.

In all, the latent-concept approach consists of four phases. First, we simulate user
sessions by randomly walking the recommendation graphs. Second, we learn concept
spaces for products in each catalogue via SVD. Third, we find a mapping between
concept-spaces using the aggregated descriptions of all products. And lastly, we
find the nearest neighbors of each product in the concept-space of the other 
catalogue concept-space. These nearest neighbors become the predicted
recommendations between products across the two catalogues. We call this method
the $LatentConcept Recommender$

\section*{Implementation}
Let us dig deeper into the details of the LSI approach described above. There 
are a number of implementation decisions that, in all, dramatically affect the
performance of this method.

\subsection*{Random Walks}
The first step of the LSI approach is to simulate user sessions by randomly
walking the recommendation graphs. There are a few ways this could be implemented.
One method is to randomly assign a random walker a starting node and then allow
it to randomly walk the edges, with a probability of termination at each step.
Running this many times is a straight-forward model of user sessions.

To speed up the simulation, we calculate a series of product-specific stationary
distributions. As with any random walk technique, we first interpret the 
recommendation graph as a transition matrix. In this walk, for each product we create a
product-specific transition matrix which has added teleportation edges back the
that product. We calculate the stationary distributions of each of these
matrices, one per product. We use these product-specific stationary
distributions as our user sessions. Since there is only one user session per 
product, we have many fewer sessions than we would with a real dataset. However,
each of these stationary distributions can be interpreted as an aggregation of
an infinite number of random walkers starting at each node and travelling until
a random termination time, as in the original model.

This process requires a couple of parameters. The first is the number of steps used to
approximate each product-specific statinary distribution. The second parameter
is the likelihood of teleporting to the special product. For our purposes we use
30 steps and a teleportation probability of 5\%.

\subsection*{Singular Value Decomposition}
Unlike Principal Compenent Analysis, standard LSI does not include any
preprocessing of the data, such as subtracting the mean from each column of the
product-session matrix. A major reason for this is that LSI is typically applied
to natural language processing problems, where the rows are terms, and there is
one term per word in the vocabulary. This otherwise sparse matrix becomes dense
if the mean is subtracted, making many NLP datasets untractable to work with. 
Our datast, on the other hand, has significantly fewer rows and columns and is
already dense. Therefore, there is little added cost to subtracting the
means. We include this preprocessing step and find that we get better results
because of it.

Another design decision is in the size of the truncation of the matrices learned
from SVD. This number dictates the number of latent concepts learned for LSI.
Too few concepts fail to capture enough detail to make accurate representations
of each product. Too many make mapping between concepts challenging. We found a
good balance at 16 concepts. 

[Graphs of concepts]

\subsection*{Concept Mapping}
A tricky operation of this procedure is the concept mapping. We attempt to find
the best linear transformation from one concept-space to the other. We do this
by representing each concept as a vector of words in the vocabulary, each
weighted with how well it represents the concept. We construct this by weighing
the tfidf of each word in a product's description by how well that word
represents the concept. Note that products can have a positive or negative value
for each concept, thus the word representation of concpets are also positive and
negative. 

With these concept representations, we construct an innner-product matrix. The 
value of a cell (i, j) in the matrix is the dot product of concept i in the first 
space with concept j in the second. The columns and rows of the resulting matrix
represent the relative similarity of the descriptive words between two concepts.
But the scale of this matrix can be very large. If the inner-product matrix is
used without any normalization as the linear transformation between the concept
spaces, the scale of the transformed products will be inconsistent with those
originally in that space. 

One way to normalize is to normalize each column of the inner-product matrix by
the L1 norm. This preserves the L1 norm of products after the transformation,
but does not preserve the L2 norm. Similarly, we can normalize the columns by
the L2 norm. Neither of these methods preserve the L2 norm, which would be ideal
as we find the nearest neighbors in the other concept-space by their L2
distance.

Another approach is to orthogonalize the inner-product matrix. That is, find the
orthogonal matrix nearest to the inner-product matrix. Nearness is defined by
the NORM FUNCTION on the difference of the two matrices. This orthogonal matrix
does perserve the L2 norm of the vectors it transforms, however the
transformation is an approximation of the ideal inner-product matrix and can
distort the relative mapping from one concept to the others.

A third approach to normalizing aims to preserve the angle of the inner-product
transformation. For this approach, we remove the issue of scale by normalizing
all products in the mapped-to space to be unit lenght. Then, after transforming
the products from the other catalogue, we normmalize them as well. Thus the
scales for both sets of products are the same and the angles of the
inner-product transformation are preserved, at the expense of discarding the
norms of all products in their product spaces. This is equivalent to calculating
the cosine similarity between products without any normalization, as cosine
similarity is indeifferent to scale. We find that this method of normalization
performs the best.

Further gains can be seen if we consider bigrams in the description as well as
the unigrams. Here, we treat bigrams as a separate set dimensions to represent
each concept. These are included in the inner product between two concepts,
weighted compared to the unigrams by a parameter $\beta$. $\beta = 0.3$ improves the
performance of the concept mapping.

\subsection*{Popularity}
The most effective addition to the LSI procedure is incorporating the popularity
of products. We do not know a priori how popular an item is as we do not have
explicit user data. However, we can use the recommendation graphs to model the
popularity of each item. The assumption is that a retailer will recommend
popular products more frequently than unpopular ones.

We imagine that recommendation graphs are constructed as follows: A retailer first
builds a similarity matrix, using a collaborative filtering method. In this
matrix, all similarity relationships are symmetric. Next, the retailer modifies
the values of the matrix to include popularity. As a simple model, we say that
the popularity-modified similarity score from Product i to Proudct j is the
simple similarity score at (i, j) multiplied by the popularity of Product j. In
this way, more popular items are more frequently recommended. 

We see signs of this in the undirected recommendation graphs. Although each product
has at most four outgoing recommendations on macys.com, we see that some
products have over one hundred incoming recommendations. With this in mind, we
approximate the popularity of each product as the logarithm of the degree of
that product in its undirected recommendation graph. We choose to model it as the
logarithm because a few of the products tend to have degrees in the hundreds.
Without some way of scaling back these popularities, these few very popular
products tend to take over all recommendations.

[Graph of popularity]

Assuming this model of recommendation graph construction, which separates similarity
from popularity, we seek to isolate similarity from the recommendation graph. Since
popular products were promoted for popularity, we demote them when walking the
graphs to create user sessions. In practice, we divide the likelihood of walking
an edge by the popularity of the product on the other side of that edge. Thus
the concepts learned on that matrix better represent products independent of
popularity. 

Proceeding as before, we find the nearest products in concept-space. However,
before deeming the nearest neighbors to be the best recommendations, we divide
the distance to each product by its popularity. Since we model distance in 
concept-space to be inversely proportional to similarity, this reincorporation
of popularity is analagous to multiplying similarity by popularity, as done in
the original construction of the recommendation graph.

A final way in which we incorporate popularity is in the number of
recommendations alotted per product. Since we model the recommendation graph as an
undirected graph, different nodes have different degrees. A product that is
popular within its catalogue is likely to be more popular across catalogues as
well. Therefore, we allow popular products to have more recommendations across
the two catalogues, proportional to their popularity.

In all, these popularity additions significantly boost performance in the
experiment. The results of this, along with the other modifications outlined in
this section, can be seen in the following section on performance.

\section*{Performance}
\subsection*{Baselines}
To understand how well our LSI recommender performs, we need good baselines
for comparison.  The simplest and easiest to beat baseline is the Random 
Recommender. This recommender randomly picks pairs of products, one from 
each catalogue, and recommends them together. This should obviously be the 
worst performing recommender but it gives us some sense of whether we are 
doing anything intelligent in our choices of recommendation.

A more impressive baseline is that of the Simple Content-Based approach, 
outlined earlier in the paper. This recommender uses information exclusively
from the descriptions of the products and ignores the recommendation graphs 
entirely. A third, somewhat similar baseline comes from the Neighborhood 
Content-based variant, which looks at the content similarity between 
neighborhoods of products.

We construct a couple of other baselines to show that our performance is not
exclusively due to any single feature of the recommender. One such baseline
tests the effects of popularity alone. Incorporating popularity has a large 
impact on the performance of any recommender and part of this is due simply 
to the evaluation metric. Any time we connect to a popular product, we are 
now two away from many more products, increasing the likelihood of getting a
correct recommendation by the 2-precision standard. To ensure that these kinds 
of effects are not the sole reason that our recommender performs well, 
we construct the Popularity Recommender. This recommender simply recommends 
that every product in one catalogue be connected to the two most popular 
products from the other catalogue. This baseline shows how much of the LSI 
recommender's performance is due simply to leveraging popularity and how 
much is due to other factors, such as intelligently learning and mapping 
latent concepts.

It is also informative to test the efficacy of the concept mapping, since this
such a hard task. As in product-product content-similarity, it is difficult to
capture similarity using descriptions alone. It is not even clear that a mapping 
necessarily exists between two seperately learned concept-spaces. The concepts 
may represent fundamentally different things, which cannot be well mapped to 
each other. We can measure the efficacy of the mapping by comparing it to a 
couple of baselines. 

The first baseline is in fact an upper bound of sorts on the mapping. This
baseline recommender is called the One Model Recommender. Unlike the other
baselines, this recommender is not a valid one, as it cheats by using
information from the unpartitioned graph. The One Model Recommender learns a
latent concept-space over the entire recommendation graph by allowing the random
walkers to traverse it before it is partitioned. In every other respect, it is
the same as the LSI Recommender. However, since products from both catalogues
are represented in the same concept-space, there is no need for a mapping
between concept-spaces.

The second mapping-related baseline is the Random Mapper Recommender. As the
name suggests, this recommender is identical to the LSI Recommender, except that
it uses a randomly generated linear transformation to map between concept
spaces. These two baselines represent (not strict) upper and lower bounds on how
well the concept mapping performs. The results show how much of a bottleneck
this step is.

\subsection*{Results}
[Number of products mapped to in different methods]
\begin{center}
\begin{tabular}{ | l | c | c | c |}
\hline
Normalization & 2-Precision & 2-Recall & 2-F1 Score \\ \hline\hline
L1 normalize each column of matrix &&&\\ \hline
L2 normalize each column of matrix &&&\\ \hline
Orthogonalize matrix &&&\\ \hline
Normalize products in concept-space &&&\\ \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ | l | c | c | c |}
\hline
Method & 2-Precision & 2-Recall & 2-F1 Score \\ \hline\hline
Random &&&\\ \hline
Simple Content-based &&&\\ \hline
Neighborhood Content-based &&&\\ \hline
One Model &&&\\ \hline
Random concept mapping &&&\\ \hline
Popularity &&&\\ \hline
LSI &&&\\ \hline
\hspace*{0.5cm} + Normalized concept mapping &&&\\ \hline
\hspace*{0.5cm} + Zero-mean SVD &&&\\ \hline
\hspace*{0.5cm} + Bigrams from descriptions &&&\\ \hline
\hspace*{0.5cm} + Discounted popularity from random walks &&&\\ \hline
\hspace*{0.5cm} + Popularity for finding nearest neighbors &&&\\ \hline
\hspace*{0.5cm} + Number of recommendations based on popularity &&&\\ \hline

\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ | l | c | c | c | c | c | c |}
\hline
Category & 1-Precision & 1-Recall & 1-F1 Score & 2-Precision & 2-Recall & 2-F1 Score \\ \hline\hline
Men's Shirts &&&&&&\\ \hline
Women's Dresses &&&&&&\\ \hline
\end{tabular}
\end{center}

Analysis/descriptions for each of these tables

\section*{Conclusion}
We tried a few different methods for creating recommendations between
retailers. We find that this one is the best.
The last method uses an unconventional approach that adds some value in the
process. These results could be used in real life for these problems, since all
of the data exists.

\section*{Future Work}
Incorporate more data such as customer reviews. Use computer vision techniques.
Focus more on popular hubs model.

\section*{Appendix}

\begin{thebibliography}{9}

\bibitem{Ricci2011}
    Francesco Ricci, Lior Rokach, and Bracha Shapira.
    \emph{Introduction to Recommender Systems Handbook}, Springer, 1-35, 2011.

\bibitem{mgp}
    For information on the Music Genome Project, visit
    http://www.pandora.com/about/mgp

\bibitem{Koren2009}
    Yehuda Koren, Robert Bell, and Chris Volinsky.
    ``Matrix factorization techniques for recommender systems.''
    \emph{Computer} 42.8, 2009. 30-37.

\bibitem{Powers2011}
    David M W Powers.
    ``Evaluation: From Precision, Recall and F-Measure to ROC, Informedness,
Markedness \& Correlation.''
    \emph{Journal of Machine Learning Technologies} \textbf{2}, 37-63, 2011.

\end{thebibliography}

\end{document}
