\documentclass[11pt]{article}

% =========================================================================
% document style changes
% =========================================================================

\usepackage{amsmath}                    % AMS math packages
\usepackage{amssymb}                    %
\usepackage[]{graphpap}
\usepackage[T1]{fontenc}                % for \mathrm{}
\usepackage{courier}                    % for \texttt{}
\usepackage{bbm}                        % for \mathbbm{1} (indicator function)
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[font={small}]{caption}
\usepackage{subcaption}

%\setlength{\parskip}{\baselineskip}     % skip line following paragraphs
%\pagestyle{empty}                       % No page numbers
\setlength{\topmargin}{-.5in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{.125in}
\setlength{\textwidth}{6.25in}

\newcommand{\spc}{\vspace{0.25in}}      % Shortcut commands
\newcommand{\ds}{\displaystyle}         %\newcommand{\ds}[1]{\displaystyle{#1}}
\newcommand{\ra}{\rightarrow}
\newcommand{\Cov}{\mathrm{Cov}}
\DeclareMathOperator*{\argmax}{arg\!\max}
\DeclareMathOperator*{\argmin}{arg\!\min}

\begin{document}                        % This is where the document begins

\title{A Technique for Inter-Catalogue Recommendations}
\author{Bill Chickering and Jamie Irvine\\
CS 399 with Anand Rajaraman\\
Stanford University}
\renewcommand{\today}{June 11, 2014}
\maketitle

\section*{Abstract}
\emph{Abstracts are abstract. They confront you. There was a reviewer a
while back who wrote that my papers didn't have any beginning or any end. He
didn't mean it as a compliment, but it was.} --Jackson Pollock

\section*{Introduction}
Recommender systems have become a critical component for online retailers in
nearly every space of retail. Whether the products are movies or clothes or
research articles, large retailers tend to have a way of recommending products
based on others previously viewed or purchased products.

These recomendations usually come from a combination of content-based
information and collaborative filtering techniques. The content-based approach
uses information about the products, such as category or description, to connect
related products. Alone, however, this approach is usually unable to capture the
subtle ways products may actually be related. Collaborative filtering, on the
other hand, learns relations from aggregated user behavior.  Such
recommendations often have explicit titles like ``People who liked this product
also liked these products''. Collaborative filtering has proven to be effective
at recommending products, given that there is a large amount of user data.

Since each retailer only has user data within their site, these user-based
recommender systems are good at learning connections between products in one
catalogue, but they do little to learn how these products may relate to other
products outside of the retailer's inventory. This type of information is
valuable in a number of situations. Retailers such as department stores want to
know which new products make the most sense to add to their catalogue.  This is
essentially a question of recommending the best outside products based on
current in-house ones. In a different scenario, two retailers may merge and want
to construct a unified recommender system between the two catalgoues.

We model this as a problem of connecting recommender systems between disjoint
catalogues. The goal is to construct good recommendations from products in one
catalogue to products in the other. These catalogues may be from competing
retailers who don't have access to each other's user data. Thus, to keep the
problem general, we do not assume that we have any private data.
Instead, we constrain ourselves to use only the product information and internal
recommendations that are posted publicly to all users.

This is a difficult problem. What makes a good recommendation is hard to define
objectively. One can say that if users co-purchase or co-view products together,
then these products should be recommended for each other. But without any user
data, it is hard to simulate this kind of user behavior. The connections between
co-viewed products are often less tangible than simple metrics of similarity. 

In this paper, we outline a technique that leverages the intra-catalogue
recommendations of each catalogue to construct inter-catalgoue recommendations
between them. We focus on clothing retailers, since they tend to all have
public recommendations for each product. Specifically, the data used for this
project was all collected from the public product webpages for Macy's.

\section*{Data}
For this work, we utilize most of the product catalog of Macy's as presented at
www.macys.com. This catalog consists of numerous categories of mostly apparel,
each of which includes hundreds to thousands of items. We confine this study to
the 49 categories within the two main parent categories ``Men'' and ``Women''
that contain at least 200 items that are associated to multiple brands. In
total, this dataset consists of 66,071 items in 49 categories with some items
listed in multiple categories. For each item we record its description, all
categories within which it is listed, and its associated recommendations. By
{\em recommendations}, we are referring to the items---there are typically four
in the case of Macy's---that are displayed on a product's details page under the
heading ``Customers Also Shopped''. The presence of these recommendations
implicitly forms a directed graph over the product catalogue, in which the nodes
are the {\em products} or {\em items} and the edges are the {\em
recommendations}. We call these types of graphs {\em recommendation graphs} and they play a central role in this study.

\section*{Experiments}
Our goal is to develop methods for connecting initially disconnected
recommendation graphs. Given two online retailers, each with a website
displaying their items along with several other {\em recommended} items from
their catalogue, we are presented with two disconnected recommendation graphs. 
We would like to associate ``good'' recommendations for items in one catalogue 
with items in the other catalogue. In this way, we are effectively introducing 
edges that connect the two recommendation graphs. Formulating this inter-retailer
recommendations problem in terms of graphs offers insight in how to evaluate our 
recommendation choices as well as how to choose ``good'' recommendations. 
Leveraging the graph structure for evaluation is discussed in this section while 
exploiting the graph for the purpose of making better recommendations is addressed 
in a subsequent section.

To evaluate our inter-retailer recommendation methods, we simulate the problem
by randomly partitioning Macy's products into two disjoint sets. Given the
original recommendation graph, each partition therefore corresponds to a graph
cut. Our premise is that an ideal recommendation method could guess the edges in
this cut with high precision and recall.

Precision, recall, and $F_1$ score are well-known metrics for evaluating
prediction algorithms.  Together, these metrics capture how effective one is at
guessing items within a target set. Consider a typical Macy's category, for
example, Dresses, which contains approximately $3,500$ items. Randomly 
partitioning results in two sets, each with about $1,750$ items. For each 
interset recommendation, we must therefore choose two nodes, one from each of the two
sets. That is, we have $1,750 \times 1,750 > 3 \times 10^6$ choices. Choosing
the correct edges is a formidable challenge to say the least. It is therefore
worth asking: Are some prediction errors better or worse than others? The graph
nature of our problem reveals that the answer is yes. It is better to guess an
edge that connects two items that are separated by two edges in the original
graph than to connect two items that are separated by five edges. We therefore
introduce the notion of {\em 2-precision}, {\em 2-recall}, and {\em 2-}$F_1$ are
defined as

\begin{align}
\text{\em 2-precision} &= \left|\left\{(u,v) \in P | d_L(u,v) \leq 2
\right\}\right|
\\\text{\em 2-recall} &= \left|\left\{(u,v) \in L | d_P(u,v) \leq 2
\right\}\right|
\\\text{\em 2-}F_1 &= 2\cdot \frac{\text{\em 2-precision} \cdot \text{\em
2-recall}}{\text{\em 2-precision} + \text{\em 2-recall}}
\end{align}
where $P$ is the set of predicted edges, $L$ is the set of lost edges (i.e.
those in the cut resulting from the partition), $d_L(u,v)$ is the shortest
distance between nodes $u$ and $v$ in the original unpartitioned graph, and 
$d_P(u,v)$ is the shortest distance between nodes $u$ and $v$ in the new graph 
formed using the predicted edges in $P$.


In these experiments, we perform the partitioning on a per-category basis. 










Next, for each category (e.g. Women Activewear, Men Dress Shirts, etc.), we
partition the products into two disjoint sets to simulate the catalogues of two
distinct retailers. Importantly, we ensure that the items of each brand are
entirely within a single partition. In this way, we preclude the easiest method
of associating two items: recongnizing a common brand. This is done to increase
the difficulty and realism of our experiment.

To make this partition realistic, we
ensure that no brand has products in both catalogue. We remove all
recommendations between products from one partition to the other, thus creating
two entirely disjoint and unconnected catalogues.

Now, we can use this partition to test a method for constructing recommendation.
For each product in each of the two catalogues, we use our recommender to choose
which products to recommend in the other catalogue. To make this problem
slightly simpler, we restrict recommendations to be within the same category,
such as Women's Dresses or Men's Shoes. In this way, connecting up entire
catalogues becomes a series of smaller tasks of connecting the products in each
category.

For this experiment, we have access to all of the true inter-catalogue
recommendations from the unpartitioned Macy's catalogue. Although we don't use
this information for constructing recommendations between the partitioned
graphs, we can use the original recommendations as a gold-standard to test the
construct ones. Using these gold-standard recommendations, we can evaluate the
precision and recall of the predicted inter-catalogue recommendations.

For the Macy's catalogue, each product has four recommendations. This means that
on average, there are only two gold-standard inter-catalogue recommendations
from each product. Since there potentially are other good
recommendations besides these two, we create another metric that expands our
notion of a correct recommendation. Here we extend the definition of a good
connection to include recommendations of recommendations in the original
catalgoue. For example, if Product B is recommended for Product A and 
Product C is recommended for Product B, we say that Product C is still a good 
recommendation for Product A. This leads us to the notions of 2-precision and 
2-recall.

For a rigorous definition of 2-precision and 2-recall, we first propose the idea
of a recommender graph. A recommender graph has products as nodes and
recommendations as edges. If Product B is recommended for Product A, there is an
edge between node A and node B in the graph. For simplicity, we define our
recommender graphs to be undirected, but we note that they could be interpreted
as directed graphs as well. With this terminology, our problem becomes one of
connecting two disjoint recommender graphs by adding edges between them. 

2-precision is defined as the percentage of predicted edges that are within two
steps in the original, unpartitioned recommender graph. In the example above,
Product A and Product C are within two steps in the original graph, thus an edge
between A and C is considered correct and increases precision. 2-recall is
defined analogously. Here, we consider the new recommender graph, consisting of
the two partitioned recommender graphs connected by the predicted
recommendations. 2-recall is then the percentage of gold-standard edges that 
are within two steps in the predicted recommender graph.

\section*{Constructing Recommendations}
Recommending products between different catalogues is a difficult task.
Typically, recommendations are constructed by leveraging user behavior
with a collaborative filtering technique. At the most basic level, If a large
number of users view or purchase both Product A and Product B, the two products
are recommended for each other. Unfortunately, for this problem there are no
user data between the two unconnected catalogues.

\subsection*{Content Based Similarity}
A simple approach is to recommend products that have similar content. Each product
has includes a short description which we can use to construct a textual 
content-based similarity function. To do this, we represent each
product as a vector of tf-idf values for each word in its descriptions. By using
tf-idf, we reduce the relevance of frequent words and boost that of rare ones.
This emphasizes the words that better explain this product versus all the other 
ones in the catalogue. The vector representations are the length of the
vocabulary. They are very sparse as most words do not appear in any given
product description. With this tf-idf represenation, we can define the 
$ContentSimiarity$ functoin as follows:

\begin{align}
ContentSimilarity(P_1, P_2) &= v_1 \cdot v_2
\end{align}
where $P_1$ and $P_2$ are the products being compared and $v_1$ and $v_2$ are
their representations as tf-idf vectors.

This similarity function yields the following recommendation algorithm which we
call the $ContentBased Recommender$: For each
product, scan through the products in the opposite catalogue and calculate the
content-based similarity score with each. Predict recommendations between this
product and each of the top k most similar products in the other catalogue.

While this is a quadric-time operation, it can be sped up using matrix
multiplication. Furthermore, the division into different categories reduces the
impact of the quadratic effect. In our case, Macy's entire catalogue consists
of over $60,000$ products, making a quadratic time algorithm very slow.
However, the average category has only one or two thousand products, which is 
significantly more manageable size for quadratic operations.

\subsection*{Leveraging Recommendations}
This $ContentBased Recommender$ is limited in a few ways. For one, it attempts
to capture similarity between products by looking exclusively at the words used
to describe them. One limitation here is that different brands often use
different vocabularies to describe the same thing, such as ``off-white'' versus
``ivory.'' Another issue is that the best recommendation for a product is often
more subtle than simply the most similar other product. For instance, a striped
black-and-white button-up shirt may be an excellent recommendation for a blue
checkered button-up, even though the two have ostensibly different patterns and
colors. 

We can alleviate some of these issues by leveraging the recommendations we
already have between products in the same catalogue. One approach is to consider
the neighborhood of each product in their recommender graphs, that is a product
along with each of the products it is recommended with in its own catalogue.
When comparing two products in different catalogues, we use the same tf-idf
similarity function from before to compare each pair of products between the two
neighborhoods. Mathematically, we construct the similarity function
NeighborhoodSimilarity defined as:

NeighborhoodSimilarity definition...

Assuming the recommendations are good within each of the catalogues, this
approach can help both of the issues brought up with the simple content-based
recommender. Since a neighborhood has many products, each describing themselves
with potentially different vocabularies, there is a higher likelihood that the
neighborhood contains multiple synonyms and thus decreases the potential of
missing due to different vocabularies. Furthermore, the recommender graphs
capture some of the user behavior not expressed in product descriptions.
Re-examining the button-up shirts example, the neighborhood of a striped shirt may
include checkered shirts and vise-versa. In this case, the two neighborhoods
would share the words ``striped'' and ``checkered'' and thus the two previously
dissimilar shirts would now be considered similar.

\subsection*{Latent Concepts}
Although the NeighborhoodSimilarity method alleviates some of the issues of
content-similarity, it is still fundamentally based on the descriptions of
products. Ideally, we would like to recommend products that are similar in more
fundamental ways than their descriptions. If a dress is elegant and modern, we
want to recommend another dress that is elegant and modern, even if those words
are not explicitly in the descriptions of either. These types of latent concepts
can be intangible and difficult to define. However, if we assume that the given
recommendations are truly based on these underlying concepts, we can attempt to 
learn the latent concepts by analyzing the graphs.

One approach to learning latent concepts is Latent Semantic Indexing or LSI. In
traditional LSI, used for Natural Language Processing, concepts are learned over 
a dataset consists of documents, each containing terms. Singular Value 
Decomposition (SVD) is applied to the term-document matrix and the resulting 
three matrices are truncated to the number of latent concepts desired. The 
resulting left matrix is a term-concept matrix, where each row is a term 
represented in concept space.

We apply the same idea to the product catalogues in order to learn the latent
concepts of each product. Analogous to documents containing terms, we look at
user sessions which contain the products viewed by that user. Although we do not
have actual user data, we can simulate user sessions by traversing the
recommender graphs. 

To simulate user sessions, we use a random walk technique on the graph. We
assume that the edges of a recommender graph indeed connect products that a user
would co-view. Thus traversing the edges of a graph is a reasonable simulation
of a user session. For details on these random walks, see the section titled
Random Walks in the Implementation section below.

With the simulated user sessions, we can implement LSI on each of the catalogues
and learn the latent concepts for each product. We want to find the products from
one catalogue that are most similar in latent concept space to products in the
other catalogue. However, since the user sessions (random walks) include
products either from one catalogue or the other, we learn different latent 
spaces for each catalogue. The problem is therefore reduced to finding a mapping
from one concept space to the other. If this can be accomplished, we
can map the concepts of each product into concepts of the other catalogue. This
creates a concept representation in the other catalouge's concept space, meaning
that we can then find nearest neighbors in that catalogue's concept space. 

To map concepts from one catalogue to another, we must revisit
content-similarity. In this case, we represent each concept as a vector of
words. To calculate the value for a word w for concept c, we sum the tf-idf
value of w across all products, weighted by how much that product is represented
by concept c. Thus if a concept could be described as formal, the words
``sophisticated'' or ``official'' may have high value since products with those
descriptive words would also have a high amount of the formal concept in their
concept representations. Using these vector representation for concepts, we can
simply calculate the dot product between two concepts to find how similar they
are, in a relative sense. This can be used to create a mapping from one set of
concepts to another.

In all, the LSI approach consists of four phases. First we simulate user
sessions by randomly walking the recommender graphs. Second we learn concept
spaces for products in each catalogue via SVD. Third we find a mapping between
concept spaces. And lastly we find the nearest neighbors of each product in the
other catalogue's concept space. These nearest neighbors are the predicted
recommendations between products in different catalogues.

\section*{Implementation}
Let us dig deeper into the details of the LSI approach described above. There 
are a number of implementation decisions that, in all, dramatically affect the
performance of this method.

\subsection*{Random Walks}
The first step of the LSI approach is to simulate user sessions by randomly
walking the recommender graphs. There are a few ways this could be implemented.
One method is to randomly assign a random walker a starting node and then allow
it to randomly walk the edges, with a probability of termination at each step.
Running this many times is a straight-forward model of user sessions.

To speed up the simulation, we calculate a series of product-specific stationary
distributions. As with any random walk technique, we first interpret the 
recommender graph as a transition matrix. In this walk, for each product we create a
product-specific transition matrix which has added teleportation edges back the
that product. We calculate the stationary distributions of each of these
matrices, one per product. We use these product-specific stationary
distributions as our user sessions. Since there is only one user session per 
product, we have many fewer sessions than we would with a real dataset. However,
each of these stationary distributions can be interpreted as an aggregation of
an infinite number of random walkers starting at each node and travelling until
a random termination time, as in the original model.

This process requires a couple of parameters. The first is the number of steps used to
approximate each product-specific statinary distribution. The second parameter
is the likelihood of teleporting to the special product. For our purposes we use
30 steps and a teleportation probability of 5\%.

\subsection*{Singular Value Decomposition}
Unlike Principal Compenent Analysis, standard LSI does not include any
preprocessing of the data, such as subtracting the mean from each column of the
product-session matrix. A major reason for this is that LSI is typically applied
to natural language processing problems, where the rows are terms, and there is
one term per word in the vocabulary. This otherwise sparse matrix becomes dense
if the mean is subtracted, making many NLP datasets untractable to work with. 
Our datast, on the other hand, has significantly fewer rows and columns and is
already dense. Therefore, there is little added cost to subtracting the
means. We include this preprocessing step and find that we get better results
because of it.

Another design decision is in the size of the truncation of the matrices learned
from SVD. This number dictates the number of latent concepts learned for LSI.
Too few concepts fail to capture enough detail to make accurate representations
of each product. Too many make mapping between concepts challenging. We found a
good balance at 16 concepts. 

[Graphs of concepts]

\subsection*{Concept Mapping}
A tricky operation of this procedure is the concept mapping. We attempt to find
the best linear transformation from one concept space to the other. We do this
by representing each concept as a vector of words in the vocabulary, each
weighted with how well it represents the concept. We construct this by weighing
the tfidf of each word in a product's description by how well that word
represents the concept. Note that products can have a positive or negative value
for each concept, thus the word representation of concpets are also positive and
negative. 

With these concept representations, we construct an innner-product matrix. The 
value of a cell (i, j) in the matrix is the dot product of concept i in the first 
space with concept j in the second. The columns and rows of the resulting matrix
represent the relative similarity of the descriptive words between two concepts.
But the scale of this matrix can be very large. If the inner-product matrix is
used without any normalization as the linear transformation between the concept
spaces, the scale of the transformed products will be inconsistent with those
originally in that space. 

One way to normalize is to normalize each column of the inner-product matrix by
the L1 norm. This preserves the L1 norm of products after the transformation,
but does not preserve the L2 norm. Similarly, we can normalize the columns by
the L2 norm. Neither of these methods preserve the L2 norm, which would be ideal
as we find the nearest neighbors in the other concept space by their L2
distance.

Another approach is to orthogonalize the inner-product matrix. That is, find the
orthogonal matrix nearest to the inner-product matrix. Nearness is defined by
the NORM FUNCTION on the difference of the two matrices. This orthogonal matrix
does perserve the L2 norm of the vectors it transforms, however the
transformation is an approximation of the ideal inner-product matrix and can
distort the relative mapping from one concept to the others.

A third approach to normalizing aims to preserve the angle of the inner-product
transformation. For this approach, we remove the issue of scale by normalizing
all products in the mapped-to space to be unit lenght. Then, after transforming
the products from the other catalogue, we normmalize them as well. Thus the
scales for both sets of products are the same and the angles of the
inner-product transformation are preserved, at the expense of discarding the
norms of all products in their product spaces. This is equivalent to calculating
the cosine similarity between products without any normalization, as cosine
similarity is indeifferent to scale. We find that this method of normalization
performs the best.

Further gains can be seen if we consider bigrams in the description as well as
the unigrams. Here, we treat bigrams as a separate set dimensions to represent
each concept. These are included in the inner product between two concepts,
weighted compared to the unigrams by a parameter $\beta$. $\beta = 0.3$ improves the
performance of the concept mapping.

\subsection*{Popularity}
The most effective addition to the LSI procedure is incorporating the popularity
of products. We do not know a priori how popular an item is as we do not have
explicit user data. However, we can use the recommendation graphs to model the
popularity of each item. The assumption is that a retailer will recommend
popular products more frequently than unpopular ones.

We imagine that recommender graphs are constructed as follows: A retailer first
builds a similarity matrix, using a collaborative filtering method. In this
matrix, all similarity relationships are symmetric. Next, the retailer modifies
the values of the matrix to include popularity. As a simple model, we say that
the popularity-modified similarity score from Product i to Proudct j is the
simple similarity score at (i, j) multiplied by the popularity of Product j. In
this way, more popular items are more frequently recommended. 

We see signs of this in the undirected recommender graphs. Although each product
has at most four outgoing recommendations on macys.com, we see that some
products have over one hundred incoming recommendations. With this in mind, we
approximate the popularity of each product as the logarithm of the degree of
that product in its undirected recommender graph. We choose to model it as the
logarithm because a few of the products tend to have degrees in the hundreds.
Without some way of scaling back these popularities, these few very popular
products tend to take over all recommendations.

[Graph of popularity]

Assuming this model of recommender graph construction, which separates similarity
from popularity, we seek to isolate similarity from the recommender graph. Since
popular products were promoted for popularity, we demote them when walking the
graphs to create user sessions. In practice, we divide the likelihood of walking
an edge by the popularity of the product on the other side of that edge. Thus
the concepts learned on that matrix better represent products independent of
popularity. 

Proceeding as before, we find the nearest products in concept space. However,
before deeming the nearest neighbors to be the best recommendations, we divide
the distance to each product by its popularity. Since we model distance in 
concept space to be inversely proportional to similarity, this reincorporation
of popularity is analagous to multiplying similarity by popularity, as done in
the original construction of the recommender graph.

A final way in which we incorporate popularity is in the number of
recommendations alotted per product. Since we model the recommender graph as an
undirected graph, different nodes have different degrees. A product that is
popular within its catalogue is likely to be more popular across catalogues as
well. Therefore, we allow popular products to have more recommendations across
the two catalogues, proportional to their popularity.

In all, these popularity additions significantly boost performance in the
experiment. The results of this, along with the other modifications outlined in
this section, can be seen in the following section on performance.

\section*{Performance}
\subsection*{Baselines}
To understand how well our LSI recommender performs, we need good baselines
for comparison.  The simplest and easiest to beat baseline is the Random 
Recommender. This recommender randomly picks pairs of products, one from 
each catalogue, and recommends them together. This should obviously be the 
worst performing recommender but it gives us some sense of whether we are 
doing anything intelligent in our choices of recommendation.

A more impressive baseline is that of the Simple Content-Based approach, 
outlined earlier in the paper. This recommender uses information exclusively
from the descriptions of the products and ignores the recommender graphs 
entirely. A third, somewhat similar baseline comes from the Neighborhood 
Content-based variant, which looks at the content similarity between 
neighborhoods of products.

We construct a couple of other baselines to show that our performance is not
exclusively due to any single feature of the recommender. One such baseline
tests the effects of popularity alone. Incorporating popularity has a large 
impact on the performance of any recommender and part of this is due simply 
to the evaluation metric. Any time we connect to a popular product, we are 
now two away from many more products, increasing the likelihood of getting a
correct recommendation by the 2-precision standard. To ensure that these kinds 
of effects are not the sole reason that our recommender performs well, 
we construct the Popularity Recommender. This recommender simply recommends 
that every product in one catalogue be connected to the two most popular 
products from the other catalogue. This baseline shows how much of the LSI 
recommender's performance is due simply to leveraging popularity and how 
much is due to other factors, such as intelligently learning and mapping 
latent concepts.

It is also informative to test the efficacy of the concept mapping, since this
such a hard task. As in product-product content-similarity, it is difficult to
capture similarity using descriptions alone. It is not even clear that a mapping 
necessarily exists between two seperately learned concept spaces. The concepts 
may represent fundamentally different things, which cannot be well mapped to 
each other. We can measure the efficacy of the mapping by comparing it to a 
couple of baselines. 

The first baseline is in fact an upper bound of sorts on the mapping. This
baseline recommender is called the One Model Recommender. Unlike the other
baselines, this recommender is not a valid one, as it cheats by using
information from the unpartitioned graph. The One Model Recommender learns a
latent concept space over the entire recommender graph by allowing the random
walkers to traverse it before it is partitioned. In every other respect, it is
the same as the LSI Recommender. However, since products from both catalogues
are represented in the same concept space, there is no need for a mapping
between concept spaces.

The second mapping-related baseline is the Random Mapper Recommender. As the
name suggests, this recommender is identical to the LSI Recommender, except that
it uses a randomly generated linear transformation to map between concept
spaces. These two baselines represent (not strict) upper and lower bounds on how
well the concept mapping performs. The results show how much of a bottleneck
this step is.

\subsection*{Results}
[Number of products mapped to in different methods]
\begin{center}
\begin{tabular}{ | l | c | c | c |}
\hline
Normalization & 2-Precision & 2-Recall & 2-F1 Score \\ \hline\hline
L1 normalize each column of matrix &&&\\ \hline
L2 normalize each column of matrix &&&\\ \hline
Orthogonalize matrix &&&\\ \hline
Normalize products in concept space &&&\\ \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ | l | c | c | c |}
\hline
Method & 2-Precision & 2-Recall & 2-F1 Score \\ \hline\hline
Random &&&\\ \hline
Simple Content-based &&&\\ \hline
Neighborhood Content-based &&&\\ \hline
One Model &&&\\ \hline
Random concept mapping &&&\\ \hline
Popularity &&&\\ \hline
LSI &&&\\ \hline
\hspace*{0.5cm} + Normalized concept mapping &&&\\ \hline
\hspace*{0.5cm} + Zero-mean SVD &&&\\ \hline
\hspace*{0.5cm} + Bigrams from descriptions &&&\\ \hline
\hspace*{0.5cm} + Discounted popularity from random walks &&&\\ \hline
\hspace*{0.5cm} + Popularity for finding nearest neighbors &&&\\ \hline
\hspace*{0.5cm} + Number of recommendations based on popularity &&&\\ \hline

\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ | l | c | c | c | c | c | c |}
\hline
Category & 1-Precision & 1-Recall & 1-F1 Score & 2-Precision & 2-Recall & 2-F1 Score \\ \hline\hline
Men's Shirts &&&&&&\\ \hline
Women's Dresses &&&&&&\\ \hline
\end{tabular}
\end{center}

Analysis/descriptions for each of these tables

\section*{Conclusion}
We tried a few different methods for creating recommendations between
retailers. We find that this one is the best.
The last method uses an unconventional approach that adds some value in the
process. These results could be used in real life for these problems, since all
of the data exists.

\section*{Future Work}
Incorporate more data such as customer reviews. Use computer vision techniques.
Focus more on popular hubs model.

\section*{Appendix}

\begin{thebibliography}{9}

\end{thebibliography}

\end{document}
